{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models:\n",
    "\n",
    "1. bloom\n",
    "2. flan-t5\n",
    "3. mT5\n",
    "4. mt0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.evaluation import DetoxificationMetrics\n",
    "from src.models import Bloom3b, FlanT5XL, MT5XL, MT0XL\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cuda:2'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data'\n",
    "\n",
    "if not os.path.isdir(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "    urllib.request.urlretrieve('https://raw.githubusercontent.com/skoltech-nlp/russe_detox_2022/main/data/input/train.tsv', os.path.join(DATA_PATH, 'train.tsv'))\n",
    "    urllib.request.urlretrieve('https://raw.githubusercontent.com/skoltech-nlp/russe_detox_2022/main/data/input/dev.tsv', os.path.join(DATA_PATH, 'dev.tsv'))\n",
    "    urllib.request.urlretrieve('https://raw.githubusercontent.com/skoltech-nlp/russe_detox_2022/main/data/input/test.tsv', os.path.join(DATA_PATH, 'test.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'en' # ru, en\n",
    "\n",
    "if lang == 'ru':\n",
    "    train_data = pd.read_csv(os.path.join(DATA_PATH, 'train.tsv'), sep='\\t').drop(columns=['index'])\n",
    "    val_data = pd.read_csv(os.path.join(DATA_PATH, 'dev.tsv'), sep='\\t')\n",
    "    df = pd.concat([train_data, val_data]).reset_index(drop=True)\n",
    "    toxic_inputs = df['toxic_comment'].tolist()\n",
    "    neutral_inputs = df['neutral_comment1'].tolist()\n",
    "elif lang == 'en':\n",
    "    df = pd.read_csv(os.path.join(DATA_PATH, 'data_en.csv'))\n",
    "    toxic_inputs = df['toxic_comment'].tolist()\n",
    "    neutral_inputs = df['neutral_comment'].tolist()\n",
    "else:\n",
    "    raise ValueError(f'Unrecognized language option. Expected one of [\"ru\", \"en\"], but got \"{lang}\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bloom3b(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where are you from? <a \">buy cialis online</a> The company said it had been forced to close its operations in the U.S. and Canada because of the shutdown. It said it would continue to operate in the U.S. and Canada, but would not be able to ship products to customers in those countries.\n",
      "I work for a publishers <a \">buy cialis online</a> The U.S. government has been trying to get the U.S. Chamber of Commerce to support the legislation, which would allow the government to buy the bonds. The Chamber has said it would not support the legislation because it would\n"
     ]
    }
   ],
   "source": [
    "print(model.generate('Where are you from?', max_length=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38af8fd09b594e7cabd8b27a2cf57b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FlanT5XL(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The king of the kings was a king of the kings. He was a king of the kings, and he was a king of the kings. One day, he was a king of the kings, and he was a king of the kings. The king of the kings was a king of the kings, and he was a king of the kings.\n"
     ]
    }
   ],
   "source": [
    "print(model.generate('Tell me a storytale, please.', max_length=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = MT5XL(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<extra_id_0>. Tell me a storytale. Tell me. Tell me.\n"
     ]
    }
   ],
   "source": [
    "print(model.generate('Tell me a storytale, please.', max_length=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f26495e1caf41f7a5b042c8a2c23aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = MT0XL(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story begins with a young girl named Hannah (Katy Perry) who is a teenager. She is a teenager who is a teenager who is a teenager who is a teenager who is a teenager who is a teenager who is a teenager who is a teenager who is a teenager who is a teenager who is a teenager who is a teenager who is a teenager who is a teenager who is a teenager who is a teenager who is a \n"
     ]
    }
   ],
   "source": [
    "print(model.generate('Tell me a storytale, please.', max_length=128))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432d222762794917ac0fe070cb6b0336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = FlanT5XL(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And, fuck, where the fuck was that fucking guy with his evidence before?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promt_template = lambda question, answer: f'Remove swear wordss {question}.\\nAnswer: {answer}\\n'\n",
    "\n",
    "num_promts = 5\n",
    "np.random.seed(42)\n",
    "ids = np.random.choice(len(toxic_inputs), num_promts, replace=False)\n",
    "text = ''.join([promt_template(toxic_inputs[i], neutral_inputs[i]) for i in ids]) + f'Rewrite text, removing swear words: {toxic_inputs[0]}.\\nAnswer: '\n",
    "\n",
    "model.generate(text, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck\n"
     ]
    }
   ],
   "source": [
    "def query_from_list(query, options):\n",
    "    t5query = f\"\"\"Question: Select the item from this list which is \"{query}\". Context: * {\" * \".join(options)}\"\"\"\n",
    "    outputs = model.generate(t5query, max_length=30)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "result = query_from_list('swear', [toxic_inputs[0]])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BloomTokenizerFast \n",
    "from petals import DistributedBloomForCausalLM\n",
    "\n",
    "\n",
    "tokenizer = BloomTokenizerFast.from_pretrained('bigscience/bloom-petals')\n",
    "model = DistributedBloomForCausalLM.from_pretrained('bigscience/bloom-petals').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewrite text, removing swear words: Yeah, time's ticking, and idiots, more and more....\n",
      "Answer: Yeah, time is ticking, and more and more stupid...\n",
      "Rewrite text, removing swear words: Everyone screamed quarantine changed that our lives would never be as fucked as they were and stayed..\n",
      "Answer: Everyone screamed quarantine changed that our lives would never continue to lie like they were stupid and stayed.\n",
      "Rewrite text, removing swear words: These assholes cut off the listing of posts through API :(.\n",
      "Answer: They've disabled the listing of posts through the API.\n",
      "Rewrite text, removing swear words: .. with their accent on your eyes, shaking the fuck out of infinity by saying \"Oh, why\" (and it looks fucking fucking funny)..\n",
      "Answer: .. from the infinity of the jaws, saying \"Okay\" (and it doesn't look great at all).\n",
      "Rewrite text, removing swear words: Which, snitcho, is typical - there's a reactor in the back that only appeared on the car in the second series..\n",
      "Answer: That there's a reactor from the back that only appeared on the car in the second series.\n",
      "Rewrite text, removing swear words: And, fuck, where the fuck was that fucking guy with his evidence before?.\n",
      "Answer: And where the fuck was that guy with his evidence before.\n",
      "Rewrite text, removing swear words: I mean, I know that the show is about the\n"
     ]
    }
   ],
   "source": [
    "promt_template = lambda question, answer: f'Rewrite text, removing swear words: {question}.\\nAnswer: {answer}\\n'\n",
    "\n",
    "num_promts = 5\n",
    "np.random.seed(42)\n",
    "ids = np.random.choice(len(toxic_inputs), num_promts, replace=False)\n",
    "text = ''.join([promt_template(toxic_inputs[i], neutral_inputs[i]) for i in ids]) + f'Rewrite text, removing swear words: {toxic_inputs[0]}.\\n'\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt')['input_ids'].to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=32)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = DetoxificationMetrics(batch_size=64, use_cuda=torch.cuda.is_available(), verbose=False, aggregate=True)\n",
    "scores = metrics(toxic_inputs, neutral_inputs)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "model.generate = partial(model.generate, max_length=20)\n",
    "metrics.evaluate_model(model, toxic_inputs, neutral_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
